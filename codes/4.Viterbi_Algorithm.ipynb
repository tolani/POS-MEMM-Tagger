{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying viterbi Algorithm : to consider tag sequence information : DP approach\n",
    "* in this notebook we ll apply viterbi algorithm on test data , and conclude with accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing necessary pacakges\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading  test featurized data matrix to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./final_df_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding some more feautres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "length = []\n",
    "for s in df['word']:\n",
    "    length.append(len(s))\n",
    "\n",
    "length_2 = np.asarray(length).reshape(-1,1)\n",
    "df['length'] = length_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hyphen = []\n",
    "all_cap = []\n",
    "sp_char = []\n",
    "cap_dot = []\n",
    "number = []\n",
    "st_alpha_dot = []\n",
    "e_day = []\n",
    "for i in df['word']:\n",
    "    if('-' in i):\n",
    "        hyphen.append(1)\n",
    "    else:\n",
    "        hyphen.append(0)\n",
    "    if(i.isupper()):\n",
    "        all_cap.append(1)\n",
    "    else:\n",
    "        all_cap.append(0)\n",
    "    if(('$' in i or ',' in i or '%' in i or '!' in i or '\\'' in i or '\"' in i) \\\n",
    "       and len(i) == 1):\n",
    "        sp_char.append(1)\n",
    "    else:\n",
    "        sp_char.append(0)\n",
    "    if(i[0].isupper() and i.endswith('.')):\n",
    "        cap_dot.append(1)\n",
    "    else:\n",
    "        cap_dot.append(0)\n",
    "    if(i[0].isdigit() and i[-1].isdigit() and ('.' in i or ',' in i)):\n",
    "        number.append(1)\n",
    "    else:\n",
    "        number.append(0)\n",
    "    if(i[0].isalpha() and '.' in i):\n",
    "        st_alpha_dot.append(1)\n",
    "    else:\n",
    "        st_alpha_dot.append(0)\n",
    "    if(i.endswith('day')):\n",
    "        e_day.append(1)\n",
    "    else:\n",
    "        e_day.append(0)\n",
    "#print('\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['hyphen'] = hyphen\n",
    "df['all_cap'] = all_cap\n",
    "df['sp_char'] = sp_char\n",
    "df['cap_dot'] = cap_dot\n",
    "df['number'] = number\n",
    "df['st_alpha_dot'] = st_alpha_dot\n",
    "df['e_day'] = e_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = df.tag.unique()\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a datamatrix 'X' and target label coln vector 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)\n",
    "\n",
    "x = data[:,2:]\n",
    "x.shape\n",
    "y = df['tag']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding prev and next context as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_cols = x.shape[1] * 5\n",
    "features = np.zeros((47380,no_cols))\n",
    "\n",
    "b = no_cols / 5\n",
    "b = int(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taking prev and next words as feature vectors : appending the stored feature vectors of the same\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    if(i==0):\n",
    "        features[i][b * 2: b* 3] = x[i]\n",
    "        features[i][b* 3:b* 4] = x[i+1]\n",
    "        features[i][b* 4:b* 5] = x[i+2]\n",
    "    \n",
    "    elif(i == 1):\n",
    "        features[i][b* 1:b* 2] = x[i-1]\n",
    "        features[i][b* 3:b* 4] = x[i+1]\n",
    "        features[i][b* 2:b* 3] = x[i]\n",
    "        features[i][b* 4:b* 5] = x[i+2]\n",
    "        \n",
    "    elif(i == x.shape[0]-1):\n",
    "        features[i][0:b* 1] = x[i-2]\n",
    "        features[i][b* 1:b* 2] = x[i-1]\n",
    "        features[i][b* 2:b* 3] = x[i]\n",
    "\n",
    "    elif(i == x.shape[0]-2):\n",
    "        features[i][b* 3:b* 4] = x[i+1]\n",
    "        features[i][0:b* 1] = x[i-2]\n",
    "        features[i][b* 1:b* 2] = x[i-1]\n",
    "        features[i][b* 2:b* 3] = x[i]\n",
    "    \n",
    "    else:\n",
    "        features[i][0:b* 1] = x[i-2]\n",
    "        features[i][b* 1:b* 2] = x[i-1]\n",
    "        features[i][b* 2:b* 3] = x[i]\n",
    "        features[i][b* 3:b* 4] = x[i+1]\n",
    "        features[i][b* 4:b* 5] = x[i+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vect = features[:-3]\n",
    "feature_vect.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "Y = le.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading trained softmax model for both c and c^2 no of classes , and also loading mapping of class label to numerical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_pairs = pickle.load(open('./pair_unigrams.pkl', 'rb'))\n",
    "bi_pairs = pickle.load(open('./pair_bigrams.pkl', 'rb'))\n",
    "soft_uni = pickle.load(open('./softmax.pkl', 'rb'))\n",
    "soft_bi = pickle.load(open('./softmax_dual.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating list of sentences on which we ll apply viterbi algo : dynamic programming approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating list of sentences \n",
    "list_sent = []\n",
    "l = []\n",
    "for word in df.word:\n",
    "    l.append(word)\n",
    "    if(word == '.'):\n",
    "        list_sent.append(l)\n",
    "        l = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = list(df['word'][1712:1742])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vect[45:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_pairs\n",
    "res = dict((v,k) for k,v in uni_pairs.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi Alogrithm Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_vect_cnt = 0\n",
    "\n",
    "total_pred_cnt = 0\n",
    "total_actual_cnt = 0\n",
    "\n",
    "for sent in list_sent:\n",
    "    \n",
    "    table = np.zeros((44,len(sent)))\n",
    "    tindex = np.zeros((44,len(sent)), dtype=int)\n",
    "\n",
    "    # taking standalone probs of first word of a sent\n",
    "    x = soft_uni.predict_proba(feature_vect[feat_vect_cnt].reshape(1,-1))\n",
    "\n",
    "    #intialization : storing standalone probs of w1 in column 1\n",
    "    for i in range(x.shape[1]):\n",
    "        table[i][0] = x[0][i]\n",
    "\n",
    "    tindex[:,0:1] = -1\n",
    "    #print(table[:,0:1].shape)\n",
    "    \n",
    "    # viterbi algorithm : dp logic\n",
    "    word = 1    # starting with 2nd coln : word2\n",
    "    while(word < len(sent)):\n",
    "        for tag in range(0,44):\n",
    "            t1name = res[tag]\n",
    "            prob2 = soft_bi.predict_proba(feature_vect[word + feat_vect_cnt].reshape(1,-1))\n",
    "            mulmax = 0\n",
    "            index = -1\n",
    "            #print(prob2[0])\n",
    "            for t2 in range(0,44):     # loop for first tag\n",
    "                tag_pair = \" \".join([res[t2],t1name])\n",
    "                    #print(tag_pair)\n",
    "                if(tag_pair in bi_pairs):           # if pair is thr then use prob else it ll store prev calcultd mulmax and index\n",
    "                    sec_term = prob2[0][bi_pairs[tag_pair]]\n",
    "                    #print(\" a \" ,sec_term)\n",
    "                    f_term = table[t2][word-1]\n",
    "                    #print(f_term)                 # pr(ti/w1)\n",
    "                    mul_term = f_term * sec_term\n",
    "                    if(mul_term > mulmax):\n",
    "                        mulmax = mul_term\n",
    "                        index = t2\n",
    "            table[tag][word] = mulmax        # storing max prob out of 44 pssble probs\n",
    "            tindex[tag][word] = index       # storing index in tindex\n",
    "\n",
    "        word += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "    print('start of sent : ',total_actual_cnt)\n",
    "    \n",
    "    \n",
    "    # storing tag sequences tht maximizes the probability in reverse order \n",
    "    j = tindex.shape[1] - 1\n",
    "    #np.max(table[:, -1:])\n",
    "    k = np.argmax(table[:, -1:])\n",
    "    tag_index = []\n",
    "    tag_index.append(k)\n",
    "    while j > 0:\n",
    "        k = tindex[k][j]\n",
    "        j -= 1\n",
    "        tag_index.append(k)\n",
    "        \n",
    "    # actual ordering\n",
    "    actual = []\n",
    "    for i in reversed(tag_index):\n",
    "        actual.append(res[i])\n",
    "    total_actual_cnt += len(actual)\n",
    "    \n",
    "    \n",
    "    # calculating accuracy\n",
    "    c = 0\n",
    "    j = feat_vect_cnt\n",
    "    for i in range(len(actual)):\n",
    "        if df.tag[j] == actual[i]:\n",
    "            c += 1\n",
    "        j += 1\n",
    "    total_pred_cnt += c\n",
    "\n",
    "    # increasing feature_vector count to starting of the next sent\n",
    "    feat_vect_cnt += len(sent)\n",
    "    \n",
    "    #print(c/len(actual))\n",
    "    \n",
    "accuracy = (total_pred_cnt/total_actual_cnt)*100\n",
    "print(\"accuracy : \" , accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy :  67.4441184541022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
